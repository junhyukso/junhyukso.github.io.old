<!doctype html><html lang=ko><head><title>ReLU6 알아보기 ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="ReLU의 변형 중 하나인 ReLU6 에 대해서 알아보겠습니다.
ReLU(Rectified Linear Unit) ReLU는 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다.
이러한 ReLU를 사용하는 가장 큰 이유는 두가지 입니다.
 비선형성 : ReLU함수가 비선형 함수이기 때문에, 뉴럴넷에 비선형성을 줄 수 있습니다. Gradient Vanishing 방지 : 양수일때, 기울기가 1이므로 Gradient가 손실되지 않습니다.  ReLU6 ReLU6는 이러한 ReLU의 변형으로, ReLU에 6이라는 상한을 걸어둔 것입니다.
MobileNet[1] 에서 주로 사용하는 Activation으로,
 Precision에서 장점을 가질 수 있습니다."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/about_relu6/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="ReLU6 알아보기"><meta name=twitter:description content="ReLU의 변형 중 하나인 ReLU6 에 대해서 알아보겠습니다."><meta property="og:title" content="ReLU6 알아보기"><meta property="og:description" content="ReLU의 변형 중 하나인 ReLU6 에 대해서 알아보겠습니다."><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/about_relu6/"><meta property="article:published_time" content="2020-10-14T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-14T00:00:00+00:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>ReLU6 알아보기</h1><div class=post-meta><span class=post-date>2020-10-14</span>
<span class=post-read-time>— 2 min read</span></div><span class=post-tags><a href=https://junhyukso.github.io/tags/deeplearning/>#DeepLearning</a>&nbsp;
<a href=https://junhyukso.github.io/tags/layer/>#Layer</a>&nbsp;</span><figure class=post-cover><img src=https://user-images.githubusercontent.com/41286195/95899022-ddc28600-0dca-11eb-8659-61f0fb31ab28.png alt="ReLU6 알아보기"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#relurectified-linear-unit>ReLU(Rectified Linear Unit)</a></li><li><a href=#relu6>ReLU6</a></li><li><a href=#실제-사용>실제 사용</a></li><li><a href=#references>References</a></li></ul></nav></aside><p>ReLU의 변형 중 하나인 ReLU6 에 대해서 알아보겠습니다.</p><h2 id=relurectified-linear-unit>ReLU(Rectified Linear Unit)</h2><p><img src=https://user-images.githubusercontent.com/41286195/95898957-c6839880-0dca-11eb-8bda-ce4953537a38.png alt><br>ReLU는 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다.</p><p>이러한 ReLU를 사용하는 가장 큰 이유는 두가지 입니다.</p><ul><li>비선형성 : ReLU함수가 비선형 함수이기 때문에, 뉴럴넷에 비선형성을 줄 수 있습니다.</li><li>Gradient Vanishing 방지 : 양수일때, 기울기가 1이므로 Gradient가 손실되지 않습니다.</li></ul><h2 id=relu6>ReLU6</h2><p><img src=https://user-images.githubusercontent.com/41286195/95899022-ddc28600-0dca-11eb-8659-61f0fb31ab28.png alt></p><p>ReLU6는 이러한 ReLU의 변형으로, ReLU에 6이라는 상한을 걸어둔 것입니다.</p><p>MobileNet[1] 에서 주로 사용하는 Activation으로,</p><ul><li>Precision에서 장점을 가질 수 있습니다. 최댓값이 6이므로, 3bit만 사용하여 표현 가능합니다.[1]</li><li>Hard한 Sigmoid로 생각할 수도 있습니다.<ul><li>실제로 MobileNet V3[3]에선 Hard-Swish 로 사용합니다.</li></ul></li><li>Sparse한 Feature를 일찍 학습할 수 있습니다. [2]</li></ul><h2 id=실제-사용>실제 사용</h2><p>ReLU6는 실제 자주 사용되는 Activation으로, 주요 프레임워크들에 대부분 구현되어 있습니다.</p><ul><li>Tensorflow : <a href=https://www.tensorflow.org/api_docs/python/tf/nn/relu6>https://www.tensorflow.org/api_docs/python/tf/nn/relu6</a></li><li>Pytorch : <a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html>https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html</a></li></ul><p>또한, 대부분의 MobileNet 구현에 사용되어 있습니다.</p><ul><li>MobileNet V1</li></ul><p>(<a href=https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/applications/mobilenet.py#L82-L310>link</a>)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_depthwise_conv_block</span>(inputs,
                          pointwise_conv_filters,
                          alpha,
                          depth_multiplier<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
                          strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>),
                          block_id<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
 <span style=color:#f92672>...</span>

  x <span style=color:#f92672>=</span> layers<span style=color:#f92672>.</span>BatchNormalization(
      axis<span style=color:#f92672>=</span>channel_axis, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;conv_dw_</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>_bn&#39;</span> <span style=color:#f92672>%</span> block_id)(
          x)
  x <span style=color:#f92672>=</span> layers<span style=color:#f92672>.</span>ReLU(<span style=color:#ae81ff>6.</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;conv_dw_</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>_relu&#39;</span> <span style=color:#f92672>%</span> block_id)(x)

 <span style=color:#f92672>...</span>

  <span style=color:#66d9ef>return</span> layers<span style=color:#f92672>.</span>ReLU(<span style=color:#ae81ff>6.</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;conv_pw_</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>_relu&#39;</span> <span style=color:#f92672>%</span> block_id)(x)
</code></pre></div><p>제가 MobileNetV1 구현을 하며 혼동을 느꼈던 부분으로, 논문에선 언급이 없지만 이때부터 ReLU6를 사용하였습니다.</p><ul><li>MobileNet V2 [1]</li></ul><p>MobileNetV2 부터는 논문에서 ReLU6를 사용했음을 언급합니다.</p><p>(<a href=https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/applications/mobilenet_v2.py>link</a>)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>...</span>
x <span style=color:#f92672>=</span> layers<span style=color:#f92672>.</span>BatchNormalization(
   axis<span style=color:#f92672>=</span>channel_axis, epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bn_Conv1&#39;</span>)(x)
x <span style=color:#f92672>=</span> layers<span style=color:#f92672>.</span>ReLU(<span style=color:#ae81ff>6.</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Conv1_relu&#39;</span>)(x)
<span style=color:#f92672>...</span> 
</code></pre></div><h2 id=references>References</h2><p>[1]Sandler, Mark, et al. &ldquo;Mobilenetv2: Inverted residuals and linear bottlenecks.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</p><p>[2] Convolutional Deep Belief Networks on CIFAR-10: Krizhevsky et al., 2010</p><p>[3] Howard, Andrew, et al. &ldquo;Searching for mobilenetv3.&rdquo; Proceedings of the IEEE International Conference on Computer Vision. 2019.</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://junhyukso.github.io/post/lottery_ticket_hypothesis/><span class=button__icon>←</span>
<span class=button__text>[REVIEW]Lottery Ticket Hypothesis</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2020 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>