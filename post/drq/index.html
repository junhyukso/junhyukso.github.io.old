<!doctype html><html lang=ko><head><title>[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration
Zhuoran Song1, Bangqi Fu1, Feiyang Wu1, Zhaoming Jiang1, Li Jiang1, Naifeng Jing1*, Xiaoyao Liang1,2 1 Shanghai Jiao Tong University, China, 2 Biren Research, China
ISCA20에 발표된 DRQ논문의 리뷰입니다.
핵심  Sentivite Region을 찾고, 그 부분만 High Precision(INT8) 으로 Quantization  Insensitive Region은 Low Preciison(INT4)   Sensitivity Region은 Inference machine에서 Dynamic하게 Runtime에 찾음  Predefine 된 Threshold보다 낮은 부분을 Insensitive Region이라고 결정   Threshold, Region Size는 Fine Tuning 하면서 결정 이러한 장점을 최대한 활용할수 있는 Hardware 또한 설계  Sensitive Region 찾기 Sensitive Values 우선 Sensitive 한 Value라는게 정말 존재할까요?"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/drq/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration"><meta name=twitter:description content="ISCA20에 발표된 DRQ논문의 리뷰입니다. DRQ는 Input Image에 따라 Dynamic 하게 Quantization을 적용합니다."><meta property="og:title" content="[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration"><meta property="og:description" content="ISCA20에 발표된 DRQ논문의 리뷰입니다. DRQ는 Input Image에 따라 Dynamic 하게 Quantization을 적용합니다."><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/drq/"><meta property="article:published_time" content="2020-12-01T01:56:20+09:00"><meta property="article:modified_time" content="2020-12-01T01:56:20+09:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration</h1><div class=post-meta><span class=post-date>2020-12-01</span>
<span class=post-read-time>— 3 min read</span></div><span class=post-tags><a href=https://junhyukso.github.io/tags/review/>#Review</a>&nbsp;
<a href=https://junhyukso.github.io/tags/deeplearning/>#DeepLearning</a>&nbsp;
<a href=https://junhyukso.github.io/tags/efficientai/>#EfficientAI</a>&nbsp;
<a href=https://junhyukso.github.io/tags/quantization/>#Quantization</a>&nbsp;</span><figure class=post-cover><img src=https://junhyukso.github.io/img/post/DRQ/cover.png alt="[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#핵심>핵심</a></li><li><a href=#sensitive-region-찾기>Sensitive Region 찾기</a><ul><li><a href=#sensitive-values>Sensitive Values</a></li><li><a href=#sensitive-region>Sensitive Region</a></li></ul></li><li><a href=#sensitive-region-찾기-알고리즘>Sensitive Region 찾기 알고리즘</a><ul><li><a href=#algorithm-overview>Algorithm Overview</a></li><li><a href=#design-space-exploration>Design Space Exploration</a></li></ul></li><li><a href=#architecture-for-drq>Architecture for DRQ</a></li><li><a href=#experimental-results>Experimental Results</a><ul><li><a href=#accuracy>Accuracy</a></li><li><a href=#performance>Performance</a></li><li><a href=#energy-consumption>Energy Consumption</a></li></ul></li></ul></nav></aside><p>DRQ: Dynamic Region-based Quantization for
Deep Neural Network Acceleration</p><p>Zhuoran Song1, Bangqi Fu1, Feiyang Wu1, Zhaoming Jiang1, Li Jiang1, Naifeng Jing1*, Xiaoyao Liang1,2
1 Shanghai Jiao Tong University, China, 2 Biren Research, China</p><p>ISCA20에 발표된 DRQ논문의 리뷰입니다.</p><h2 id=핵심>핵심</h2><ul><li>Sentivite Region을 찾고, 그 부분만 High Precision(INT8) 으로 Quantization<ul><li>Insensitive Region은 Low Preciison(INT4)</li></ul></li><li>Sensitivity Region은 Inference machine에서 Dynamic하게 Runtime에 찾음<ul><li>Predefine 된 Threshold보다 낮은 부분을 Insensitive Region이라고 결정</li></ul></li><li>Threshold, Region Size는 Fine Tuning 하면서 결정</li><li>이러한 장점을 최대한 활용할수 있는 Hardware 또한 설계</li></ul><h2 id=sensitive-region-찾기>Sensitive Region 찾기</h2><h3 id=sensitive-values>Sensitive Values</h3><p><img src=/img/post/DRQ/sensitivity_values.png alt="Sensitive values">
우선 Sensitive 한 Value라는게 정말 존재할까요? 저자들은 이름 검증하기 위해 아래와 같은 실험을 수행했습니다.<br>Input Featrue Map을 세 그룹으로 나눕니다. 나누는 기준은 Input의 Magnitude를 기준으로,<br>상위 20%, 20~80%, 80% 입니다.<br>그리고 각각의 그룹에 노이즈를 적용합니다. 예를들어, 상위 20%에만 노이즈를 줬다면 TFF<br>상위 20%,20~80%에만 줬다면 TTF입니다.<br><strong>Observation from experiment</strong></p><ul><li>TFF가 FTF,FFT보다 Drop이 급격함.</li><li>TFF ,TFT,TTF,TTT가 거의 일정. 즉, 상위20%그룹의 영향이 가장 큼.</li><li>FFT는 큰 노이즈를 허용함.</li><li>Imagenet이 Cifar10보다 좀 더 Noise에 민감.</li></ul><p>즉, Input의 <strong>Magintude가 클수록 Sensitive</strong> 함을 확인할 수 있습니다.</p><h3 id=sensitive-region>Sensitive Region</h3><p><img src=/img/post/DRQ/sensitivity_region.png alt="Sensitive Region">
그렇다면 이러한 Sensitive Value들의 분포는 어떻게 될까요?<br>저자들은 이를 우선 Visualize 하기 위해, LeNet5에 MNIST데이터셋에 대해 실험을 수행했습니다.<br>위 그림은 LeNet5에 3이란 이미지를 넣었을때, 처음 3 레이어를 나타낸 그림입니다.</p><p>결과적으로, Magnitude가 큰 Group(Segment 0)은 무작위적으로 분포하지 않고, <strong>집합하는 경향</strong>이 있음을 알 수 있습니다. Segment 2는 중요하지 않은 부분에 넓게 분포함을 확인할 수 있습니다.</p><p>그렇다면, 이러한 Input Feature Map의 h<em>w를 x</em>y개의 Patch로 나눠서 , Sensitive/ Insensitive한 Patch로 나눌 수 있을 것입니다. 이를 In/Sensitive Region 라고 부릅니다.</p><h2 id=sensitive-region-찾기-알고리즘>Sensitive Region 찾기 알고리즘</h2><p>우선 DRQ Algorithm의 Overview를 잠깐 집고 넘어갑시다.</p><h3 id=algorithm-overview>Algorithm Overview</h3><p><img src=/img/post/DRQ/DRQ_algo_overview.png alt="DRQ Overview"></p><ol><li>Sensitive Region Predictor</li></ol><p>Sensitive Region Predictor는 Runtime에, Input Feature Map의 각 Region들이 Sensitive한지 판단하는 역할을 합니다.</p><ul><li>h*w사이즈의 Input Feature Map을 받아서, 우선 FP32→INT8로 Quantize 합니다.<ul><li>Int8이 기본 Precision 입니다.</li></ul></li><li>IFM을 x*y 개의 region으로 분할합니다.</li><li>각각의 Region을 Mean Filtering 합니다</li><li>이를 <strong>Predifiend Threshold Activation에 통과 시킵니다.</strong><ul><li>Threshold 를 Predefine하는 방법은 밑의 Design Space Exploration절에서 설명합니다.</li></ul></li><li>이를 통해 각 Region에 대해 Binary Mask를 만듭니다. Binary Mask의 dimension은 (h<em>w)/(x</em>y) 입니다.</li></ul><ol start=2><li>Mixed Precision Convolution
<img src=/img/post/DRQ/mixed_precision_conv.png alt="Mixed Precision Convolution">
Mixed Precision Convolution은 Binary Mask에 따라, 각각 다른 precision으로 Convolution을 진행합니다.</li></ol><ul><li>우선 Kernel Weights들은 INT8로 Qunatize하여 DRAM에 저장합니다.</li><li>Sensitive Region과 연산할때는, 일반 INT8 Convolution을 사용합니다.</li><li>INSensitive Region으로 연산할때는 Input이 INT4로 DRAM에 저장됩니다. Convolution시는, Weights를 바로 INT4범위로 <strong>Clipping</strong> 해서 INT4 Conv를 수행합니다.</li></ul><h3 id=design-space-exploration>Design Space Exploration</h3><p>두가지 DSE요소가 있습니다.</p><ul><li><p>첫째는 Threshold 입니다.<br>Threshold가 클수록 당연히 속도향상이 클것이지만(Insensitive Region이 많아지므로), 중요한 피쳐를 버릴 가능성이 높아지므로 ACC drop이 클것입니다.</p></li><li><p>두번째는 Region(x*y)의 크기 입니다.<br>Region이 작을 수록 세세한 Input Region을 보는것이므로 정확도로의 영향이 작겠지만, 가속효과가 적을것입니다. 또한 Region의 크기는 HW Friendly해야합니다.</p></li><li><p>또한, 이러한 Mixed Precision Convolution을 진행하게되면 Weight가 원래 사용하던 Input이 아니므로 Acc drop이 클 것입니다.</p></li></ul><p>저자들은 이를 해결하기위해 간단한 Finetuning 방법을 제시합니다.</p><ul><li>Threshold와 Region Size를 우선 적당히 큰값으로 잡습니다.</li><li>이러한 Threshold와 Region Size를 적용한 Forward Pass를 Mixed Precision으로 수행합니다.</li><li>단, Backward Pass는 Full Precision으로 진행합니다.</li><li>이를 정확도가 수렴할때까지 진행합니다.</li><li>최종 목표 정확도가 나올때까지 Threshold와 Region Size를 더 줄여(1/2) 다시 진행합니다.</li><li>위의 과정은 몇 Iter 만에 끝나기때문에 빠르게 가능합니다.</li></ul><p>실제 서로다른 네트워크들에 대해 찾아지는 Threshold와 Region Size의 예시는 아래 그림과 같습니다.
<img src=/img/post/DRQ/result_th_reg.png alt="real threshold and region size"></p><h2 id=architecture-for-drq>Architecture for DRQ</h2><p>이부분은 일단 HW에 대한 지식이 부족하여 스킵합니다.</p><h2 id=experimental-results>Experimental Results</h2><h3 id=accuracy>Accuracy</h3><p><img src=/img/post/DRQ/result_acc.png alt="result acc"></p><h3 id=performance>Performance</h3><p><img src=/img/post/DRQ/result_time.png alt="result time"></p><h3 id=energy-consumption>Energy Consumption</h3><p><img src=/img/post/DRQ/result_power.png alt="result Energy"></p><p>실험결과 가장 빠르고, 저전력이면서도 정확도가 기존 방법들에 비해 떨어지지 않음을 확인할 수 있습니다.</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://junhyukso.github.io/post/tf_keras_layer_wrapper/><span class=button__icon>←</span>
<span class=button__text>[TF2.0]keras Layer Wrapping 하기</span></a></span>
<span class="button next"><a href=https://junhyukso.github.io/post/install_tor_on_ubuntu/><span class=button__text>Ubuntu에서 Tor 사용하기</span>
<span class=button__icon>→</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2021 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>