<!doctype html><html lang=ko><head><title>[REVIEW]Lottery Ticket Hypothesis ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Network Pruning Pruning은 딥러닝 모델을 경량화 하기 위한 방법으로, 정확도 손실을 최소로 하며 모델에서 어느 정도의 파라미터들을 제거하는 기법입니다.
Iterative Pruning 가장 널리 사용되는 Pruning 방법인 Iterative Pruning은 위 그림과 같습니다.
우선 어떠한 기준을 통해 중요하지 않은 파라미터를 판단하고, 제거합니다. 그리고 모델을 다시 재학습시킵니다.
이러한 step들을 반복함으로써, 모델의 파라미터를 점점 더 제거합니다.
Problem 하지만, 이러한 방식을 통해 얻은 SubNetwork를, Randomly Initialize한후, 처음부터 학습시키게 되면 본래의 성능을 달성할 수 없었습니다.
Iterative Pruning자체가 꽤 많은 HyperParamter들이 있고, SubNetwork를 학습시킬수있다면 Train FLOPs또한 큰 폭으로 줄일 수 있기에 이 문제를 해결하는 것은 중요했습니다."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/lottery_ticket_hypothesis/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="[REVIEW]Lottery Ticket Hypothesis"><meta name=twitter:description content="Lottery Ticket Hyphothesis에 대해 알아봅시다."><meta property="og:title" content="[REVIEW]Lottery Ticket Hypothesis"><meta property="og:description" content="Lottery Ticket Hyphothesis에 대해 알아봅시다."><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/lottery_ticket_hypothesis/"><meta property="article:published_time" content="2020-10-19T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-19T00:00:00+00:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>[REVIEW]Lottery Ticket Hypothesis</h1><div class=post-meta><span class=post-date>2020-10-19</span>
<span class=post-read-time>— 2 min read</span></div><span class=post-tags><a href=https://junhyukso.github.io/tags/review/>#Review</a>&nbsp;
<a href=https://junhyukso.github.io/tags/deeplearning/>#DeepLearning</a>&nbsp;
<a href=https://junhyukso.github.io/tags/efficient-dl/>#Efficient DL</a>&nbsp;
<a href=https://junhyukso.github.io/tags/pruning/>#Pruning</a>&nbsp;</span><figure class=post-cover><img src=https://user-images.githubusercontent.com/41286195/96431539-9dd22780-123e-11eb-8a54-e9bcc60f93d2.png alt="[REVIEW]Lottery Ticket Hypothesis"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#network-pruning>Network Pruning</a><ul><li><a href=#iterative-pruning>Iterative Pruning</a></li><li><a href=#problem>Problem</a></li></ul></li><li><a href=#the-lottery-ticket-hyphotesis>The Lottery Ticket Hyphotesis</a></li><li><a href=#experimental-results>Experimental Results</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></aside><h2 id=network-pruning>Network Pruning</h2><p>Pruning은 딥러닝 모델을 경량화 하기 위한 방법으로, <strong>정확도 손실을 최소로</strong> 하며 모델에서 어느 정도의 <strong>파라미터들을 제거</strong>하는 기법입니다.</p><h3 id=iterative-pruning>Iterative Pruning</h3><p><img src=https://user-images.githubusercontent.com/41286195/96431539-9dd22780-123e-11eb-8a54-e9bcc60f93d2.png alt="Iterative Pruning. Han et al 2015"></p><p>가장 널리 사용되는 Pruning 방법인 Iterative Pruning은 위 그림과 같습니다.<br>우선 어떠한 기준을 통해 <strong>중요하지 않은</strong> 파라미터를 판단하고, 제거합니다. 그리고 모델을 다시 <strong>재학습</strong>시킵니다.<br>이러한 step들을 반복함으로써, 모델의 파라미터를 점점 더 제거합니다.</p><h3 id=problem>Problem</h3><p>하지만, 이러한 방식을 통해 얻은 SubNetwork를, Randomly Initialize한후, <strong>처음부터 학습</strong>시키게 되면 본래의 성능을 달성할 수 없었습니다.<br>Iterative Pruning자체가 꽤 <strong>많은 HyperParamter</strong>들이 있고, SubNetwork를 학습시킬수있다면 <strong>Train FLOPs또한 큰 폭으로 줄일 수 있기에</strong> 이 문제를 해결하는 것은 중요했습니다.</p><p>The Lotter Ticket Hyphothesis[ICLR2019] 에서 저자들은 이러한 문제를 해결할 수 있는 방법을 제시합니다.</p><h2 id=the-lottery-ticket-hyphotesis>The Lottery Ticket Hyphotesis</h2><p>우선 Lottery Ticket이란 용어부터 정의합니다.</p><ul><li>Lottery Ticket : <em>Original Network</em> 보다 <strong>적은 Parameter</strong>를 가지고, <strong>성능또한 더 좋은</strong> <em>SubNetwork</em>.</li><li>저자들은 이를 말그대로 <strong>복권</strong>에 비유해, Lottery Ticket이라는 용어를 사용했습니다.</li></ul><p>논문에서 제시하는 이러한 <strong>Lottery ticket</strong>을 찾는 방법은 아래와 같습니다.
<img src=https://user-images.githubusercontent.com/41286195/96432957-0968c480-1240-11eb-8b5a-33fb3ec394cb.png alt="Finding Lottery ticket"></p><ul><li>1,2,3과정은 통상적인 Train -> Iterative Pruning 과정입니다.</li><li><strong>4, 이제 Iterative Pruning으로 찾은 SubNetowrk를 1에서 사용했던 &ldquo;초기값"으로 초기화 합니다.</strong><ul><li>이때, 초기값에 사용한 분포는 Xavier와 같은 일반적인 분포입니다.</li></ul></li></ul><p>이러한 과정을 통해 찾은 Lottery Ticket을, <strong>처음부터 학습시킨 결과</strong>는 아래와 같습니다.</p><h2 id=experimental-results>Experimental Results</h2><p><img src=https://user-images.githubusercontent.com/41286195/96435049-8eec7480-1240-11eb-913a-16a016071808.png alt="results on LeNet">
십자가 기호 옆의 숫자는 기존 네트워크 대비 남아있는 파라미터의 비율입니다.</p><ul><li>제일 왼쪽 그림을 보게되면, 기존 네트워크(100)보다 Lottery ticket들의 학습결과가 월등함을 알 수 있습니다.</li><li>가운데 그림을 보게되면, 3.6%(보라색)까지도 기존 네트워크보다 학습결과가 좋지만, 1.9%(갈색)은 결과가 크게 나빠지는 것을 볼 수 있다.</li><li>이러한 SubNetwork의 파라미터 수의 어떠한 하한이 있다는 것을 알 수 있습니다.</li><li>오른쪽 그림을 보게되면, Lottery ticket방법을 적용한 결과가, 적용하지 않은 결과(reinit)보다 월등히 좋음을 확인할 수 있습니다.</li></ul><p>저자들은 Simple Convnet이나 Deep Convnet(VGG,ResNet..)에 대해서도 실험을 진행하였는데, 몇가지 휴리스틱이 들어가긴했지만 모두 좋은 결과를 보였습니다.</p><h2 id=conclusion>Conclusion</h2><h2 id=references>References</h2><p>Frankle, Jonathan, and Michael Carbin. &ldquo;The lottery ticket hypothesis: Finding sparse, trainable neural networks.&rdquo; arXiv preprint arXiv:1803.03635 (2018).</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://junhyukso.github.io/post/making_hugo_blog/><span class=button__icon>←</span>
<span class=button__text>HUGO블로그 시작하기</span></a></span>
<span class="button next"><a href=https://junhyukso.github.io/post/about_relu6/><span class=button__text>ReLU6 알아보기</span>
<span class=button__icon>→</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2020 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>