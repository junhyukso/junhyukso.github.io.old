<!doctype html><html lang=ko><head><title>[TF2.0]keras Layer Wrapping 하기 ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="실험을 위해서, 이미 존재하는 keras모델의 특정레이어들에서, Input/Output Tensor들에 약간의 조작이 필요한일이 생겼습니다. 따라서 Layer를 특정 Wrapper로 감싸려고 하였는데요,
처음에는 단순히 기존 모델생성코드를 수정하여, 사이사이 Wrapper Function을 추가하는걸 생각해봤습니다.
하지만 레이어가 수십개라면 너무 귀찮은 작업이고 코드의 범용성도 떨어지기에 이참에 범용적으로 적용가능하게 코드를 작성하려고 해봤습니다.
우선 생각해보니, tfmot패키지의 prune_low_magnitude함수가 이러한 작업을 하던것이 기억나, 우선 tfmot의 소스코드의 분석부터 진행해 보았습니다.
TFMOT 소스코드 분석 우선 prune_low_magnitude() 함수부터 시작합니다.
prune_low_magnitude(model,&amp;hellip;) 소스코드
def _add_pruning_wrapper(layer): if isinstance(layer, pruning_wrapper."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/tf_keras_layer_wrapper/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="[TF2.0]keras Layer Wrapping 하기"><meta name=twitter:description content="TF2 keras에서 모델의 Layer들에 Wrapper를 씌우는법을 알아봅시다."><meta property="og:title" content="[TF2.0]keras Layer Wrapping 하기"><meta property="og:description" content="TF2 keras에서 모델의 Layer들에 Wrapper를 씌우는법을 알아봅시다."><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/tf_keras_layer_wrapper/"><meta property="article:published_time" content="2020-12-01T21:31:40+09:00"><meta property="article:modified_time" content="2020-12-01T21:31:40+09:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>[TF2.0]keras Layer Wrapping 하기</h1><div class=post-meta><span class=post-date>2020-12-01</span>
<span class=post-read-time>— 5 min read</span></div><span class=post-tags><a href=https://junhyukso.github.io/tags/programming/>#Programming</a>&nbsp;
<a href=https://junhyukso.github.io/tags/deeplearning/>#DeepLearning</a>&nbsp;
<a href=https://junhyukso.github.io/tags/tensorflow/>#Tensorflow</a>&nbsp;
<a href=https://junhyukso.github.io/tags/keras/>#Keras</a>&nbsp;</span><figure class=post-cover><img src=https://junhyukso.github.io/img/post/tf_keras_layer_wrapper/cover.png alt="[TF2.0]keras Layer Wrapping 하기"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#tfmot-소스코드-분석>TFMOT 소스코드 분석</a><ul><li><a href=#prune_low_magnitudemodel>prune_low_magnitude(model,&mldr;)</a></li><li><a href=#classprunelowmagnitudekeraslayerswrapper>Class:PruneLowMagnitude(keras.layers.Wrapper)</a></li><li><a href=#class--keraslayerswrapper>class : keras.layers.Wrapper</a></li><li><a href=#classprunelowmagnitudekeraslayerswrapper-분석>Class:PruneLowMagnitude(keras.layers.Wrapper) 분석</a></li></ul></li><li><a href=#결론>결론</a></li></ul></nav></aside><p>실험을 위해서, 이미 존재하는 keras모델의 특정레이어들에서, Input/Output Tensor들에 약간의 조작이 필요한일이 생겼습니다. 따라서 Layer를 특정 Wrapper로 감싸려고 하였는데요,</p><p>처음에는 단순히 기존 모델생성코드를 수정하여, 사이사이 Wrapper Function을 추가하는걸 생각해봤습니다.</p><p>하지만 레이어가 수십개라면 너무 귀찮은 작업이고 코드의 범용성도 떨어지기에 이참에 범용적으로 적용가능하게 코드를 작성하려고 해봤습니다.</p><p>우선 생각해보니, tfmot패키지의 prune_low_magnitude함수가 이러한 작업을 하던것이 기억나, 우선 tfmot의 소스코드의 분석부터 진행해 보았습니다.</p><h2 id=tfmot-소스코드-분석>TFMOT 소스코드 분석</h2><p>우선 prune_low_magnitude() 함수부터 시작합니다.</p><h3 id=prune_low_magnitudemodel>prune_low_magnitude(model,&mldr;)</h3><p><a href=https://github.com/tensorflow/model-optimization/blob/a6b401c28d7f0ee5d36f31b00d34c75a36e08362/tensorflow_model_optimization/python/core/sparsity/keras/prune.py>소스코드</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_add_pruning_wrapper</span>(layer):
    <span style=color:#66d9ef>if</span> isinstance(layer, pruning_wrapper<span style=color:#f92672>.</span>PruneLowMagnitude):
      <span style=color:#66d9ef>return</span> layer
    <span style=color:#66d9ef>return</span> pruning_wrapper<span style=color:#f92672>.</span>PruneLowMagnitude(layer, <span style=color:#f92672>**</span>params)

<span style=color:#f92672>...</span>

<span style=color:#66d9ef>elif</span> is_sequential_or_functional:
    <span style=color:#66d9ef>return</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>clone_model(
        to_prune, input_tensors<span style=color:#f92672>=</span>None, clone_function<span style=color:#f92672>=</span>_add_pruning_wrapper)
</code></pre></div><p>핵심은 이부분인데, 보시게 되면
keras.models.clone_model의 clone_function인자를 사용하여 Wrapper function을 주고 있습니다.</p><p>wrapper function으로 PruneLowMagnitude란 클래스에 레이어를 인자로 주고 있는데, 이 클래스를 보게 되면 다음과 같습니다.</p><h3 id=classprunelowmagnitudekeraslayerswrapper>Class:PruneLowMagnitude(keras.layers.Wrapper)</h3><p><a href=https://github.com/tensorflow/model-optimization/blob/e8e5266b96498fe6b1807665ce26ca8f0213b2f0/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py>소스코드</a></p><ul><li>결론적인 방법은 <strong>결론 절</strong>에 적어두었습니다</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>Wrapper <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Wrapper

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PruneLowMagnitude</span>(Wrapper):
  _PRUNE_CALLBACK_ERROR_MSG <span style=color:#f92672>=</span> (
      <span style=color:#e6db74>&#39;Prune() wrapper requires the UpdatePruningStep callback to be provided &#39;</span>
      <span style=color:#e6db74>&#39;during training. Please add it as a callback to your model.fit call.&#39;</span>)

  <span style=color:#66d9ef>def</span> __init__(self,
               layer,
               pruning_schedule<span style=color:#f92672>=</span>pruning_sched<span style=color:#f92672>.</span>ConstantSparsity(<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0</span>),
               block_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>),
               block_pooling_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;AVG&#39;</span>,
               <span style=color:#f92672>**</span>kwargs):
    <span style=color:#75715e>#내용생략</span>

    <span style=color:#66d9ef>if</span> isinstance(layer, prunable_layer<span style=color:#f92672>.</span>PrunableLayer):
      <span style=color:#75715e># Custom layer in client code which supports pruning.</span>
      super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>__init__(layer, <span style=color:#f92672>**</span>kwargs)
    <span style=color:#66d9ef>elif</span> prune_registry<span style=color:#f92672>.</span>PruneRegistry<span style=color:#f92672>.</span>supports(layer):
      <span style=color:#75715e># Built-in keras layers which support pruning.</span>
      super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>__init__(
          prune_registry<span style=color:#f92672>.</span>PruneRegistry<span style=color:#f92672>.</span>make_prunable(layer), <span style=color:#f92672>**</span>kwargs)
    <span style=color:#66d9ef>else</span>:
      <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(
          <span style=color:#e6db74>&#39;Please initialize `Prune` with a supported layer. Layers should &#39;</span>
          <span style=color:#e6db74>&#39;either be a `PrunableLayer` instance, or should be supported by the &#39;</span>
          <span style=color:#e6db74>&#39;PruneRegistry. You passed: {input}&#39;</span><span style=color:#f92672>.</span>format(input<span style=color:#f92672>=</span>layer<span style=color:#f92672>.</span>__class__))

    <span style=color:#75715e>#내용생략</span>

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(self, input_shape):
    super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>build(input_shape)
		self<span style=color:#f92672>.</span>add_variable( some_needed_variable )
		
		<span style=color:#75715e>#내용생략</span>

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs, training<span style=color:#f92672>=</span>None):
    <span style=color:#66d9ef>if</span> training <span style=color:#f92672>is</span> None:
      training <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>learning_phase()

    <span style=color:#75715e>#내용생략</span>

    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;training&#39;</span> <span style=color:#f92672>in</span> args:
      <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs, training<span style=color:#f92672>=</span>training)

    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs)

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_output_shape</span>(self, input_shape):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>compute_output_shape(input_shape)

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trainable</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>trainable

  <span style=color:#a6e22e>@trainable.setter</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trainable</span>(self, value):
    self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>trainable <span style=color:#f92672>=</span> value

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trainable_weights</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>trainable_weights

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>non_trainable_weights</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>non_trainable_weights <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>_non_trainable_weights

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>updates</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>updates <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>_updates

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>losses</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>losses <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>_losses

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_weights</span>(self):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>get_weights()

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_weights</span>(self, weights):
    self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>set_weights(weights)
</code></pre></div><p>위와 같은 구조를 하고 있습니다. 소스코드를 좀 많이 쳐냈는데, 밑에서 자세히 옮겨놨기도 하고Wrapper의 사용법이 궁금한거지 Pruning 하는 방법이 궁금한게 아니므로 쳐냈습니다.</p><h3 id=class--keraslayerswrapper>class : keras.layers.Wrapper</h3><p>이 클래스는<a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Wrapper>keras.layers.Wrapper</a>란 Abstract Class를 Implement 하고 있습니다.</p><p>이 Abstract class는 <a href=https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/wrappers.py#L40-L81>이 소스코드</a> 에서 확인할 수 있는데, 큰 내용이 있진 않습니다.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Wrapper</span>(Layer):
  <span style=color:#e6db74>&#34;&#34;&#34;Abstract wrapper base class.
</span><span style=color:#e6db74>  Wrappers take another layer and augment it in various ways.
</span><span style=color:#e6db74>  Do not use this class as a layer, it is only an abstract base class.
</span><span style=color:#e6db74>  Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.
</span><span style=color:#e6db74>  Arguments:
</span><span style=color:#e6db74>    layer: The layer to be wrapped.
</span><span style=color:#e6db74>  &#34;&#34;&#34;</span>

  <span style=color:#66d9ef>def</span> __init__(self, layer, <span style=color:#f92672>**</span>kwargs):
    <span style=color:#66d9ef>assert</span> isinstance(layer, Layer)
    self<span style=color:#f92672>.</span>layer <span style=color:#f92672>=</span> layer
    super(Wrapper, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(self, input_shape<span style=color:#f92672>=</span>None):
    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>built:
      self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>build(input_shape)
      self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>built <span style=color:#f92672>=</span> True
    self<span style=color:#f92672>.</span>built <span style=color:#f92672>=</span> True

  <span style=color:#a6e22e>@property</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>activity_regularizer</span>(self):
    <span style=color:#66d9ef>if</span> hasattr(self<span style=color:#f92672>.</span>layer, <span style=color:#e6db74>&#39;activity_regularizer&#39;</span>):
      <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>activity_regularizer
    <span style=color:#66d9ef>else</span>:
      <span style=color:#66d9ef>return</span> None

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_config</span>(self):
    config <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;layer&#39;</span>: generic_utils<span style=color:#f92672>.</span>serialize_keras_object(self<span style=color:#f92672>.</span>layer)}
    base_config <span style=color:#f92672>=</span> super(Wrapper, self)<span style=color:#f92672>.</span>get_config()
    <span style=color:#66d9ef>return</span> dict(list(base_config<span style=color:#f92672>.</span>items()) <span style=color:#f92672>+</span> list(config<span style=color:#f92672>.</span>items()))

  <span style=color:#a6e22e>@classmethod</span>
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>from_config</span>(cls, config, custom_objects<span style=color:#f92672>=</span>None):
    <span style=color:#f92672>from</span> tensorflow.python.keras.layers <span style=color:#f92672>import</span> deserialize <span style=color:#66d9ef>as</span> deserialize_layer  <span style=color:#75715e># pylint: disable=g-import-not-at-top</span>
    <span style=color:#75715e># Avoid mutating the input dict</span>
    config <span style=color:#f92672>=</span> copy<span style=color:#f92672>.</span>deepcopy(config)
    layer <span style=color:#f92672>=</span> deserialize_layer(
        config<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#39;layer&#39;</span>), custom_objects<span style=color:#f92672>=</span>custom_objects)
    <span style=color:#66d9ef>return</span> cls(layer, <span style=color:#f92672>**</span>config)
</code></pre></div><ul><li>중요한 부분은, self.layer에 원본 레이어를 저장한다는 것입니다.</li></ul><h3 id=classprunelowmagnitudekeraslayerswrapper-분석>Class:PruneLowMagnitude(keras.layers.Wrapper) 분석</h3><p>keras.Wrapper class가 따로 Document가 존재하지 않아서 PruneLowMagnitude의 실제 구현에서 사용방법을 살펴볼 필요가 있습니다.</p><p>우리가 Wrapper Class를 구현할때 가장 크게 관심가질 부분은</p><ul><li><strong>init</strong></li><li>build</li><li>call</li></ul><p>입니다. 그부분의 핵심 코드를 확인하면 다음과 같습니다.</p><p><strong>**Init</strong>**</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>  <span style=color:#66d9ef>if</span> isinstance(layer, prunable_layer<span style=color:#f92672>.</span>PrunableLayer):
      <span style=color:#75715e># Custom layer in client code which supports pruning.</span>
      super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>__init__(layer, <span style=color:#f92672>**</span>kwargs)
    <span style=color:#66d9ef>elif</span> prune_registry<span style=color:#f92672>.</span>PruneRegistry<span style=color:#f92672>.</span>supports(layer):
      <span style=color:#75715e># Built-in keras layers which support pruning.</span>
      super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>__init__(
          prune_registry<span style=color:#f92672>.</span>PruneRegistry<span style=color:#f92672>.</span>make_prunable(layer), <span style=color:#f92672>**</span>kwargs)
    <span style=color:#66d9ef>else</span>:
</code></pre></div><p>Wrapper class를 layer넣어서 Init 해주는 걸로 충분해보입니다.</p><p><strong>Build</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(self, input_shape):
    super(PruneLowMagnitude, self)<span style=color:#f92672>.</span>build(input_shape)

    weight_vars, mask_vars, threshold_vars <span style=color:#f92672>=</span> [], [], []

    self<span style=color:#f92672>.</span>prunable_weights <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>get_prunable_weights()

    <span style=color:#75715e># For each of the prunable weights, add mask and threshold variables</span>
    <span style=color:#66d9ef>for</span> weight <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>prunable_weights:
      mask <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_variable(
          <span style=color:#e6db74>&#39;mask&#39;</span>,
          shape<span style=color:#f92672>=</span>weight<span style=color:#f92672>.</span>shape,
          initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>initializers<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;ones&#39;</span>),
          dtype<span style=color:#f92672>=</span>weight<span style=color:#f92672>.</span>dtype,
          trainable<span style=color:#f92672>=</span>False,
          aggregation<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>VariableAggregation<span style=color:#f92672>.</span>MEAN)
      threshold <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_variable(
          <span style=color:#e6db74>&#39;threshold&#39;</span>,
          shape<span style=color:#f92672>=</span>[],
          initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>initializers<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;zeros&#39;</span>),
          dtype<span style=color:#f92672>=</span>weight<span style=color:#f92672>.</span>dtype,
          trainable<span style=color:#f92672>=</span>False,
          aggregation<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>VariableAggregation<span style=color:#f92672>.</span>MEAN)

      weight_vars<span style=color:#f92672>.</span>append(weight)
      mask_vars<span style=color:#f92672>.</span>append(mask)
      threshold_vars<span style=color:#f92672>.</span>append(threshold)
    self<span style=color:#f92672>.</span>pruning_vars <span style=color:#f92672>=</span> list(zip(weight_vars, mask_vars, threshold_vars))

    <span style=color:#75715e># Add a scalar tracking the number of updates to the wrapped layer.</span>
    self<span style=color:#f92672>.</span>pruning_step <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_variable(
        <span style=color:#e6db74>&#39;pruning_step&#39;</span>,
        shape<span style=color:#f92672>=</span>[],
        initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>initializers<span style=color:#f92672>.</span>Constant(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>),
        dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int64,
        trainable<span style=color:#f92672>=</span>False)
</code></pre></div><p><code>super(PruneLowMagnitude, self).build(input_shape)</code> 로 원래 layer의 build를 호출해주고,</p><p>Wrapper에서 따로 필요한 Variables를 할당하는것으로 충분해보입니다.</p><p><strong>Call</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs, training<span style=color:#f92672>=</span>None):
    <span style=color:#66d9ef>if</span> training <span style=color:#f92672>is</span> None:
      training <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>learning_phase() <span style=color:#75715e>#Training 상태가 명시되지 않았을 경우 Keras Backend에서 읽어옴.</span>

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_update</span>():
      <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>control_dependencies([
          tf<span style=color:#f92672>.</span>debugging<span style=color:#f92672>.</span>assert_greater_equal(
              self<span style=color:#f92672>.</span>pruning_step,
              np<span style=color:#f92672>.</span>int64(<span style=color:#ae81ff>0</span>),
              message<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>_PRUNE_CALLBACK_ERROR_MSG)
      ]):
        <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>control_dependencies(
            [self<span style=color:#f92672>.</span>pruning_obj<span style=color:#f92672>.</span>conditional_mask_update()]):
          <span style=color:#66d9ef>return</span> tf<span style=color:#f92672>.</span>no_op(<span style=color:#e6db74>&#39;update&#39;</span>)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>no_op</span>():
      <span style=color:#66d9ef>return</span> tf<span style=color:#f92672>.</span>no_op(<span style=color:#e6db74>&#39;no_update&#39;</span>)

    update_op <span style=color:#f92672>=</span> utils<span style=color:#f92672>.</span>smart_cond(training, add_update, no_op)
    self<span style=color:#f92672>.</span>add_update(update_op)
    <span style=color:#75715e># Always execute the op that performs weights = weights * mask</span>
    <span style=color:#75715e># Relies on UpdatePruningStep callback to ensure the weights</span>
    <span style=color:#75715e># are sparse after the final backpropagation.</span>
    <span style=color:#75715e>#</span>
    <span style=color:#75715e># self.add_update does nothing during eager execution.</span>
    self<span style=color:#f92672>.</span>add_update(self<span style=color:#f92672>.</span>pruning_obj<span style=color:#f92672>.</span>weight_mask_op())
    <span style=color:#75715e># TODO(evcu) remove this check after dropping py2 support. In py3 getargspec</span>
    <span style=color:#75715e># is deprecated.</span>
    <span style=color:#66d9ef>if</span> hasattr(inspect, <span style=color:#e6db74>&#39;getfullargspec&#39;</span>):
      args <span style=color:#f92672>=</span> inspect<span style=color:#f92672>.</span>getfullargspec(self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call)<span style=color:#f92672>.</span>args
    <span style=color:#66d9ef>else</span>:
      args <span style=color:#f92672>=</span> inspect<span style=color:#f92672>.</span>getargspec(self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call)<span style=color:#f92672>.</span>args
    <span style=color:#75715e># Propagate the training bool to the underlying layer if it accepts</span>
    <span style=color:#75715e># training as an arg.</span>
    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;training&#39;</span> <span style=color:#f92672>in</span> args:
      <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs, training<span style=color:#f92672>=</span>training)

    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs)
</code></pre></div><p>이것도 역시 <code>self.layer.call(inputs)</code> 만 해주면 되는것으로 보이네요.</p><h2 id=결론>결론</h2><p>분석해보니 필요한 구현은 세가지 입니다.</p><ul><li>keras.Wrapper class를 Implement한 WrapperClass 생성<ul><li>필수는 아니지만, 공식 Abstract Class가 있으니 따릅시다.</li></ul></li><li>어떤 Layer을 Wrap할지 체크하는 함수</li><li>이 clone function를 keras.clone_model( clone_function인자에 전달)</li></ul><p>제가 사용하는 구현의 의사코드는 다음과 같습니다.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyWrapper</span>(keras<span style=color:#f92672>.</span>Wrapper):
	<span style=color:#66d9ef>def</span> __init__(self,layer, some_options<span style=color:#f92672>...</span> ):
		super(MyWrapper, self)<span style=color:#f92672>.</span>__init__(layer, <span style=color:#f92672>**</span>kwargs)
		<span style=color:#f92672>...</span> initialize some other stuff <span style=color:#f92672>...</span>
	
	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(self, input_shape):
		super(MyWrapper, self)<span style=color:#f92672>.</span>build(input_shape)
		self<span style=color:#f92672>.</span>addVariable(<span style=color:#e6db74>&#39;some_needed_variable&#39;</span>, <span style=color:#f92672>...</span> )
		<span style=color:#f92672>...</span>

	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, input, training<span style=color:#f92672>=</span>None):
		<span style=color:#66d9ef>if</span> training <span style=color:#f92672>is</span> None:
	    training <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>learning_phase()

		<span style=color:#e6db74>&#34;&#34;&#34;DO SOMETHING WHEN CALLING&#34;&#34;&#34;</span>

		<span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;training&#39;</span> <span style=color:#f92672>in</span> args:
            <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs, training<span style=color:#f92672>=</span>training)
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>layer<span style=color:#f92672>.</span>call(inputs)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_wrapper_func</span>(layer):
	<span style=color:#66d9ef>if</span> layer <span style=color:#f92672>is</span> some_condition :
		<span style=color:#66d9ef>return</span> MyWrapper(layer, some_option<span style=color:#f92672>...</span>)
	<span style=color:#66d9ef>else</span> :
		<span style=color:#66d9ef>return</span> layer
		
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_unwrapper_func</span>(layer):
	<span style=color:#66d9ef>if</span> isinstance(layer,MyWrapper):
		<span style=color:#66d9ef>return</span> layer<span style=color:#f92672>.</span>layer <span style=color:#75715e>#keras.Wrapper클래스는 self.layer에 원본레이어 저장</span>
	<span style=color:#66d9ef>else</span> :
		<span style=color:#66d9ef>return</span> layer

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wrap_model</span>(model):
	<span style=color:#66d9ef>return</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>clone_model(
        model, input_tensors<span style=color:#f92672>=</span>None, clone_function<span style=color:#f92672>=</span>_wrapper_func)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>unwrap_model</span>(model):
	<span style=color:#66d9ef>return</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>clone_model(
        model, input_tensors<span style=color:#f92672>=</span>None, clone_function<span style=color:#f92672>=</span>_unwrapper_func)

</code></pre></div><p>제 실험에서는, Training시만 Wrapper가 필요하고 Inference시에는 필요없으므로 Unwrap_model 함수또한 만들어 두었습니다.</p><p>감사합니다.</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://junhyukso.github.io/post/numpy_1d_interpolation/><span class=button__icon>←</span>
<span class=button__text>Numpy로 1D 배열 Interpolation 하기</span></a></span>
<span class="button next"><a href=https://junhyukso.github.io/post/drq/><span class=button__text>[REVIEW] DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration</span>
<span class=button__icon>→</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2020 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>