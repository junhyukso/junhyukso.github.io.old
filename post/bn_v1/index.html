<!doctype html><html lang=ko><head><title>Batch Normalization 정리 ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Intro Batch Normalization(이하 BN)은 딥러닝 모델을 학습시킬때 사용되는 레이어중 하나로,
2015년에 발표된 이후로 그 성능을 인정받아 현재까지도 매우 활발하게 사용되는 중입니다.
(엄청난 인용수&amp;hellip;)
BN은 딥러닝 모델을 훈련할시 수렴의 안정성과 속도 향상을 가져옵니다.
이 포스팅에서는 BN레이어의 동작과, 여러가지 응용을 살펴보겠습니다. BN의 모티베이션이나 상세한 실험 결과들은 해당논문 (https://arxiv.org/pdf/1502.03167.pdf)을 참조해주세요.
Batch Normalization BN 레이어는 내부적으로 네가지 Parameter를 가집니다.
 Mean Variance Gamma Beta  BN레이어의 동작자체는 위 그림과 같이 간단합니다.
 레이어의 인풋을 Mean,Variance로 정규화 후 gamma를 곱해준후, beta를 더해줍니다."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/bn_v1/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="Batch Normalization 정리"><meta name=twitter:description content="BN레이어의 동작과, 몇가지 응용을 알아보겠습니다."><meta property="og:title" content="Batch Normalization 정리"><meta property="og:description" content="BN레이어의 동작과, 몇가지 응용을 알아보겠습니다."><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/bn_v1/"><meta property="article:published_time" content="2020-12-26T16:55:39+09:00"><meta property="article:modified_time" content="2020-12-26T16:55:39+09:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>Batch Normalization 정리</h1><div class=post-meta><span class=post-date>2020-12-26</span>
<span class=post-read-time>— 2 min read</span></div><span class=post-tags><a href=https://junhyukso.github.io/tags/deeplearning/>#DeepLearning</a>&nbsp;
<a href=https://junhyukso.github.io/tags/layer/>#Layer</a>&nbsp;
<a href=https://junhyukso.github.io/tags/efficientai/>#EfficientAI</a>&nbsp;</span><figure class=post-cover><img src=https://junhyukso.github.io/img/post/bn_v1/cover.png alt="Batch Normalization 정리"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#batch-normalization>Batch Normalization</a><ul><li><a href=#gamma-beta>Gamma, Beta</a></li><li><a href=#mean--variance>Mean , Variance</a></li><li><a href=#training-mode>Training Mode</a></li><li><a href=#inference-mode>Inference Mode</a></li></ul></li><li><a href=#batch-normalization-fusing>Batch Normalization Fusing</a></li><li><a href=#batch-renormalization>Batch Renormalization</a></li><li><a href=#references>References</a></li></ul></nav></aside><h2 id=intro>Intro</h2><p>Batch Normalization(이하 BN)은 딥러닝 모델을 학습시킬때 사용되는 레이어중 하나로,</p><p>2015년에 발표된 이후로 그 성능을 인정받아 현재까지도 매우 활발하게 사용되는 중입니다.</p><p><img src=/img/post/bn_v1/god.png alt="엄청난 인용수">
(엄청난 인용수&mldr;)</p><p>BN은 딥러닝 모델을 훈련할시 수렴의 안정성과 속도 향상을 가져옵니다.<br>이 포스팅에서는 BN레이어의 동작과, 여러가지 응용을 살펴보겠습니다. BN의 모티베이션이나 상세한 실험 결과들은 해당논문 (<a href=https://arxiv.org/pdf/1502.03167.pdf>https://arxiv.org/pdf/1502.03167.pdf</a>)을 참조해주세요.</p><h2 id=batch-normalization>Batch Normalization</h2><p>BN 레이어는 내부적으로 <strong>네가지 Parameter</strong>를 가집니다.</p><ul><li>Mean</li><li>Variance</li><li>Gamma</li><li>Beta</li></ul><p><img src=/img/post/bn_v1/algo1.png alt="BN Algorithm"></p><p>BN레이어의 동작자체는 위 그림과 같이 간단합니다.</p><ul><li>레이어의 인풋을 Mean,Variance로 정규화 후</li><li>gamma를 곱해준후, beta를 더해줍니다.</li></ul><h3 id=gamma-beta>Gamma, Beta</h3><p>이때 Gamma와 Beta는 Trainable한 Parameter로, 딥러닝 모델의 학습시 <strong>오차역전파</strong>를 통해 적절한 값으로 학습됩니다.</p><p>Gamma는 1, Beta는 0으로 처음 초기화 되게 됩니다.</p><h3 id=mean--variance>Mean , Variance</h3><p>그러나 Mean, Variance는 <strong>오차역전파를 통해 학습되는 값이 아닙니다</strong>. 또한 Training mode와 Inference mode시에 사용하는 방식이 다릅니다.</p><h3 id=training-mode>Training Mode</h3><p>Training mode에서는 레이어의 인풋으로 들어온 <strong>Mini Batch의 Mean, Variance</strong>를 사용하여 정규화를 진행합니다.</p><p>이때, Mean, Varaince를 한번 사용하고 버리는 것이 아니고</p><p><strong>Inference Mode에서의 사용을 위해 계속 지수이동평균으로 축적합니다.</strong></p><p>지수이동평균은 다음과 같은 식으로 계산됩니다.</p><p>이때 alpha는 <strong>Momentum</strong>이란 계수로 보통 0.9, 0.99, 0.999와 같이 1에 가까운 값을 사용합니다.</p><p>Tensorflow, Pytorch 구현에서도 기본값으로 0.99,0.9를 사용함을 확인할 수 있습니다.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>BatchNormalization(
    axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.99</span>, epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>, center<span style=color:#f92672>=</span>True, scale<span style=color:#f92672>=</span>True,
    beta_initializer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;zeros&#39;</span>, gamma_initializer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ones&#39;</span>,
    moving_mean_initializer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;zeros&#39;</span>,
    moving_variance_initializer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ones&#39;</span>, beta_regularizer<span style=color:#f92672>=</span>None,
    gamma_regularizer<span style=color:#f92672>=</span>None, beta_constraint<span style=color:#f92672>=</span>None, gamma_constraint<span style=color:#f92672>=</span>None,
    renorm<span style=color:#f92672>=</span>False, renorm_clipping<span style=color:#f92672>=</span>None, renorm_momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.99</span>, fused<span style=color:#f92672>=</span>None,
    trainable<span style=color:#f92672>=</span>True, virtual_batch_size<span style=color:#f92672>=</span>None, adjustment<span style=color:#f92672>=</span>None, name<span style=color:#f92672>=</span>None, <span style=color:#f92672>**</span>kwargs
) <span style=color:#75715e>#TF Momentum: 0.99</span>

torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>BatchNorm2d(
	num_features, 
	eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, 
	momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, 
	affine<span style=color:#f92672>=</span>True, 
	track_running_stats<span style=color:#f92672>=</span>True
) <span style=color:#75715e>#Torch Momentum: 0.9(0.1)</span>
</code></pre></div><h3 id=inference-mode>Inference Mode</h3><p>Inference mode에서는 이러한 방식을 통해,</p><ul><li>지수이동평균으로 계산된 Mean, Var</li><li>오차역전파를 통해 찾아진 Gamma , Beta</li></ul><p>를 사용하여 BN계산을 진행합니다.</p><p>이미 <strong>고정된 값만을 사용하여 진행하기 때문에, O(1)로 계산이 가능합니다.</strong></p><h2 id=batch-normalization-fusing>Batch Normalization Fusing</h2><p>CNN모델에서 BN레이어는 보통,</p><ul><li><strong>채널 단위로 존재하고</strong></li><li>Convolution → BN → ReLU의 순서로 구성됩니다.</li></ul><p>따라서, Inference Mode라면 Convolution → BN 연산은</p><p><strong>y = Gamma * (( Wconv * x ) - Mean )/Var + Beta</strong></p><p>입니다. 이는</p><p><strong>y = W*x + b</strong></p><p>와 같이 단순화 시킬 수 있습니다.</p><p>따라서 실제 추론기 구현시, Batch Normalization레이어를 구성하지 않더라도,</p><p><strong>Convolution 레이어의 Weight ,bias를 조작하여 똑같은 계산 결과가 나오게 두 오퍼레이션을 합칠(Fusing)수 있습니다</strong>.</p><p>이러한 최적화 방식을 BN Fusing이라고 부릅니다.</p><p><strong>Inference Mode</strong>에서의 **BN은 O(1)**이므로, 상당히 <strong>Memory Bound되는 연산</strong>입니다.</p><p><strong>BN Fusing</strong>을 하게 되면, Conv이후 BN레이어를 수행하기 위해 <strong>RAM에 중간 Activation을 저장해야 할 필요가 없습니다.</strong></p><p>따라서 <strong>램의 Bandwidth를 아껴 추론시 상당한 수행시간의 이득을 볼 수 있는 효과</strong>가 있습니다.</p><p>실제 프레임워크에서들에서는 CBR을 Fusing하는 최적화를 내부적으로 모두 진행하고 있습니다.</p><h2 id=batch-renormalization>Batch Renormalization</h2><p>[TODO]</p><h2 id=references>References</h2><ul><li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</li><li><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization>https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a></li><li><a href=https://nenadmarkus.com/p/fusing-batchnorm-and-conv/>https://nenadmarkus.com/p/fusing-batchnorm-and-conv/</a></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://junhyukso.github.io/post/patdnn/><span class=button__icon>←</span>
<span class=button__text>[REVIEW]PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning</span></a></span>
<span class="button next"><a href=https://junhyukso.github.io/post/deebert/><span class=button__text>[REVIEW] DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</span>
<span class=button__icon>→</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2021 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>