<!doctype html><html lang=ko><head><title>[REVIEW] RepVGG:Making VGG-style ConvNets Great Again ::
Junhyuk So's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Make VGG Great Again!
Intro CNN의 모델은 성능향상을 위해 구조가 점점 복잡해지고 있습니다. Branch가 중간에 존재하기도 하고,(ResNet) 1x1 Conv와 Depthwise Convolution(MobileNet)을 사용하기도 하고, 비선형성을 위해 H-Swish와 같은 activation을 사용하기도 합니다.
RepVGG는 이러한 기법들 없이(정확히는 Inference 시만), 오직 3x3Conv와 ReLU만을 사용하여 Imagenet Top1 Acc 80%라는 인상적인 결과를 보여주었습니다.
Training VGG와 같은 단순한 ConvNet구조는 Training시 Gradient Vanishing이라던가 하는 문제로 성능이 좋지 않다는 문제가 있습니다. 따라서 ResNet처럼 Residual Branch를 만들어, Gradient를 더 잘 흐르게 하여 성능향상을 이뤄내는 경우가 많아졌습니다."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://junhyukso.github.io/post/repvgg/><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6LJX3GCJZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-Q6LJX3GCJZ');</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=stylesheet href=https://junhyukso.github.io/assets/style.css><link rel=stylesheet href=https://junhyukso.github.io/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://junhyukso.github.io/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://junhyukso.github.io/img/favicon.png><link href=https://junhyukso.github.io/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=https://junhyukso.github.io/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="[REVIEW] RepVGG:Making VGG-style ConvNets Great Again"><meta name=twitter:description content="Make VGG Great Again!"><meta property="og:title" content="[REVIEW] RepVGG:Making VGG-style ConvNets Great Again"><meta property="og:description" content="Make VGG Great Again!"><meta property="og:type" content="article"><meta property="og:url" content="https://junhyukso.github.io/post/repvgg/"><meta property="article:published_time" content="2021-01-15T21:09:25+09:00"><meta property="article:modified_time" content="2021-01-15T21:09:25+09:00"><meta property="og:site_name" content="Junhyuk So's Blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archive>Archive</a></li><li><a href=/tags>Tags</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>[REVIEW] RepVGG:Making VGG-style ConvNets Great Again</h1><div class=post-meta><span class=post-date>2021-01-15</span>
<span class=post-read-time>— 2 min read</span></div><figure class=post-cover><img src=https://junhyukso.github.io/img/post/RepVGG/fig2.png alt="[REVIEW] RepVGG:Making VGG-style ConvNets Great Again"></figure><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#training>Training</a></li><li><a href=#reparameterization>Reparameterization</a></li><li><a href=#winograd-convolution>Winograd Convolution</a></li><li><a href=#experimental-result>Experimental Result</a></li><li><a href=#reference>Reference</a></li></ul></nav></aside><p>Make VGG Great Again!</p><h2 id=intro>Intro</h2><p><img src=/img/post/RepVGG/fig1.png alt=fig1></p><p>CNN의 모델은 성능향상을 위해 <strong>구조가 점점 복잡</strong>해지고 있습니다. <strong>Branch가 중간에 존재</strong>하기도 하고,(ResNet) <strong>1x1 Conv와 Depthwise Convolution</strong>(MobileNet)을 사용하기도 하고, 비선형성을 위해 <strong>H-Swish</strong>와 같은 activation을 사용하기도 합니다.</p><p>RepVGG는 이러한 기법들 없이(정확히는 Inference 시만), <strong>오직 3x3Conv와 ReLU만을 사용</strong>하여 Imagenet Top1 Acc 80%라는 인상적인 결과를 보여주었습니다.</p><h2 id=training>Training</h2><p><img src=/img/post/RepVGG/fig2.png alt=fig2></p><p>VGG와 같은 단순한 ConvNet구조는 <strong>Training시 Gradient Vanishing이라던가 하는 문제로 성능이 좋지 않다는 문제</strong>가 있습니다. 따라서 ResNet처럼 Residual Branch를 만들어, Gradient를 더 잘 흐르게 하여 성능향상을 이뤄내는 경우가 많아졌습니다.</p><p>RepVGG는 여기서 영감을 받아, <strong>Training</strong> 시에는 1x1Conv와 Identitiy로 이루어진 <strong>Residual branch</strong>를 사용하고, <strong>Inference</strong> 시에는 이를 reparameterization기법으로 <strong>하나의 3x3Conv</strong>로 합치는 기법을 제시하였습니다.</p><h2 id=reparameterization>Reparameterization</h2><p><img src=/img/post/RepVGG/fig3.png alt=fig3></p><p>RepVGG에서는 각 3x3Conv옆에, 1x1Conv와 Identity branch를 추가했습니다.</p><p>이들은 ReLU를 통과하지 않고 합쳐지므로(중간에 비선형성이 없으므로), <strong>하나의 Conv로 합칠수 있습니다.</strong></p><p>과정은 아래와 같습니다.</p><ul><li>우선 <strong>Identity branch</strong>는 Identity Matrix를 Kernel로 가지는 1x1 Conv와 동치입니다.</li><li>또한, <strong>1x1Conv</strong>는 상하좌우 패딩이 1씩 들어간 3x3Conv와 동치입니다.</li><li>또한, <strong>Conv-BN</strong>은 BN Folding을 통해 하나의 Conv, bias로 생각할수 있습니다.</li><li>이제 각각 <strong>세개의 3x3Conv를 더해서, 하나의 3x3 Conv</strong>로 만듭니다.</li></ul><h2 id=winograd-convolution>Winograd Convolution</h2><p>사실 <strong>Branch</strong>가 없어지는 것은 <strong>Memory적 이득</strong> 요소가 크고, <strong>Addition</strong>은 실제 <strong>수행시간의 영향이 크지 않으</strong>므로 VGG형태의 CNN을 사용한다해서 <strong>속도의 차이는 크게 없습</strong>니다.</p><p>하지만 <strong>3x3Conv만 사용하여 이득</strong>을 얻을 수 있는 부분이 있는데요, 바로 <strong>Winograd Convolution</strong>입니다.</p><p><strong>Winograd Conv</strong>는 Strassen 행렬곱의 Conv버전이라고도 생각할 수 있는데요, 핵심은 <strong>곱하기 연산의 수를 줄이고 더하기 연산의 수를 늘리는 것</strong>입니다.</p><p>보통 하드웨어적으로, <strong>곱셈기의 코스트</strong>가 <strong>덧셈기보다 훨씬 크기</strong>때문에 이러한 개선은 <strong>수행시간적으로 의미있는 향상</strong>이 있습니다.</p><p>3x3Conv의 경우엔 <strong>곱셈연산의 수가 4/9</strong>로 줄어듭니다.</p><p>자세한 설명은 해당 논문(<a href=https://arxiv.org/pdf/1509.09308.pdf>Lavin et al.</a>)를 참조해주세요.</p><h2 id=experimental-result>Experimental Result</h2><p><img src=/img/post/RepVGG/fig4.png alt=fig4></p><p><strong>Reparameterization기법</strong>을 통해,<strong>VGG Style CNN을 유지</strong>하면서 SOTA는 아니지만 <strong>정확도와 속도향상을 모두 이뤄냈다는 점</strong>에서 흥미로운 논문이였던 것 같습니다.</p><p>감사합니다.</p><h2 id=reference>Reference</h2><p><a href=https://arxiv.org/pdf/2101.03697v1.pdf>https://arxiv.org/pdf/2101.03697v1.pdf</a></p><p><a href=https://arxiv.org/pdf/1509.09308.pdf>https://arxiv.org/pdf/1509.09308.pdf</a></p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=https://junhyukso.github.io/post/patdnn/><span class=button__text>[REVIEW]PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning</span>
<span class=button__icon>→</span></a></span></div></div><script src=https://utteranc.es/client.js repo=junhyukso/junhyukso.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Junhyuk So's Blog</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2021 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://junhyukso.github.io/assets/main.js></script><script src=https://junhyukso.github.io/assets/prism.js></script></div></body></html>